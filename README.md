ğŸ­ Sarcasm Detection using Multimodal Speech Processing
ğŸ“Œ Overview
This project aims to detect sarcasm in speech using deep learning and multimodal feature extraction. It leverages three powerful pre-trained models:

Facebook Wav2Vec2 XLS-R-1B â€“ Extracts deep speech representations.
Microsoft UniSpeech-SAT-Base-100h-Libri-Fit â€“ Enhances speech embeddings with speaker-aware tuning.
ImageBind â€“ Enables multimodal integration.
We process audio embeddings from context and utterance separately, apply noise reduction techniques (PCA), feature selection (SelectKBest), and deep learning with fully connected networks.

ğŸš€ Features
âœ”ï¸ Uses pretrained speech models to extract robust audio embeddings.
âœ”ï¸ Applies PCA to remove noise from embeddings.
âœ”ï¸ Implements SelectKBest for feature selection.
âœ”ï¸ Adds delta features to capture sarcasm variations.
âœ”ï¸ Uses a deep neural network with dropout and batch normalization for better generalization.
âœ”ï¸ Optimized with AdamW optimizer instead of Adam for better convergence.
âœ”ï¸ Model checkpointing ensures the best epoch is used for final testing.

ğŸ“‚ Dataset
Input: Audio speech samples labeled as sarcastic or non-sarcastic.
Features extracted using Wav2Vec2 XLS-R-1B, UniSpeech-SAT, and ImageBind.
Processed features stored as .csv files.

ğŸ— Model Architecture
The model consists of two branches:

Context Branch â€“ Processes previous context speech features.
Utterance Branch â€“ Processes the current utterance features.
Fusion Layer â€“ Combines both branches and classifies sarcasm.
